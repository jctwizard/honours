{
  "name": "HonoursJournal",
  "tagline": "Intelligent user interfaces for virtual reality",
  "body": "### Hello World *16/09/16*\r\nAfter listening to the various pieces of advice given to us I decided the best way to find a topic for my proposal would be to find a problem I would enjoy researching and solving. Having closely followed developments in Virtual Reality over the past years I know that user interfaces still have many issues associated with them. When using any controller with positional tracking it is harder to design a single interface for every user, due to different heights, arm lengths and dexterity. Thus my preliminary concept for a topic is to make a user interface back end that can easily adapt and intelligently evolve based on the user.\r\n\r\n### Research *26/09/16*\r\nI spent a good few days trawling through ACM and the library search uncovering various books along with the conference, IUI (Intelligent User Interfaces). I contacted the professor who runs the conference to request the location of the previous papers and was pleasantly surprised to receive a quick reply directing me toward the proceedings of the last 21 conferences. I also began doing some user research (probably not the term) by borrowing the key to the HIVE in Abertay which is filled with various virtual reality headsets. The next step is to start finding literature that lends itself to the implementation of my project. In the back of my head I have been considering the various platforms I could use and frameworks. I would really like to avoid using an existing game engine as my project application will not need much more than a renderer to function. I am starting to lean toward implementing a Virtual Reality camera for the Game Education Framework.\r\n\r\n### Refining the Topic *02/10/16*\r\nI had a small scare when speaking with a lecturer about my topic about the validity of my research question as a technical problem. Fortunately this was resolved when I further discussed the details of my proposal and although I have not posed a technical problem it will certainly have a technical solution. I have also altered my topic to cover Adaptive User Interfaces for Motion Controlled Virtual Reality as opposed to Adaptive and Intelligent User Interfaces. This is because Adaptive User Interfaces could be applied more readily for short user sessions whereas an intelligent user interface would require a number of longer sessions. After more research there seem to be two main 'semantics' when it comes to adaptivity for user interfaces, static and dynamic. Static adaptivity focuses on updating and optimising the user experience between sessions while dynamic uses techniques to continuously adjust the interface for the user. \r\n\r\nAfter speaking with a lecturer I am starting to consider implementing my application in Unreal Engine or Unity but obviously focusing on the programming of the user interface and avoiding their systems for implementing such things. They persuaded me that this would allow more focus on the development of the adaptivity. The Playstation VR headset available for research purposes would allow me to develop for motion controlled virtual reality without competing with the many 3rd year group projects using the Vive (the other motion controlled VR platform available to us). I also emailed someone at Oculus about the potential of an Oculus Touch developer kit but have not heard back. The next step is to begin setting up a project and pipeline for Unity/Unreal and the PSVR and work out how I can develop an application without the constant need for the headset as it will be shared and I may not always have access.\r\n\r\n### Evaluating existing interfaces *06/10/16*\r\nI spent a few hours exploring the various interfaces of the available Vive games on the University's machine. I noted some interesting features. Almost every game uses highlighting to display when something will be interacted with. This creates a fairly robust sense of responsiveness when quickly performing tasks. However when many items are selectable the highlighted object isn't always the obvious one at least in Fantastic Contraption and Valve's The Lab. Another point of interest was that the majority of my errors were made when trying to pick things up from the floor or on surfaces. Although it seems counterintuitive the interfaces hovering in the air which don't really have a real world reference are consistently easier to use. Although it does become tiring using elevated interfaces.\r\n\r\nMost of the experiences utilised only one button on the controller outside of very specific circumstances in which the controller changed to suit the interaction, i.e. the touchpad became a scroll wheel in VR to display that interacting would scroll the visible text. I thought thoroughly about how to execute the practical side of the project as well as the evaluation aspect. By creating an iteration of an interface which can then be evaluated through testing seems like the obvious choice. The first iteration would be a typical 2D interface like many of the menus found in VR games. The second iteration would be a 3D interface which utilises static adaptivity. i.e. will adapt during transition sequences like going back or forward through a menu. The final iteration would use dynamic adaptivity to create a continuously adjusting environment to accommodate as best as possible to the user.\r\n\r\nThe evaluation would be done by presenting the user with the three environments asking them to perform a series of interactions and then evaluate how usable the interfaces were. This would most likely also ask them to evaluate how much error they made using the interfaces. As well as having quality testing I plan to implement quantitive testing by gathering data on how many times attempted interactions fail or are undone. This should provide a good idea of how well the adaptive elements of the program affect the usability.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}