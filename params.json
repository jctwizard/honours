{
  "name": "HonoursJournal",
  "tagline": "Intelligent user interfaces for virtual reality",
  "body": "### *16/09/16* Hello World\r\nAfter listening to the various pieces of advice given to us I decided the best way to find a topic for my proposal would be to find a problem I would enjoy researching and solving. Having closely followed developments in Virtual Reality over the past years I know that user interfaces still have many issues associated with them. When using any controller with positional tracking it is harder to design a single interface for every user, due to different heights, arm lengths and dexterity. Thus my preliminary concept for a topic is to make a user interface back end that can easily adapt and intelligently evolve based on the user.\r\n\r\n### *26/09/16* Research\r\nI spent a good few days trawling through ACM and the library search uncovering various books along with the conference, IUI (Intelligent User Interfaces). I contacted the professor who runs the conference to request the location of the previous papers and was pleasantly surprised to receive a quick reply directing me toward the proceedings of the last 21 conferences. I also began doing some user research (probably not the term) by borrowing the key to the HIVE in Abertay which is filled with various virtual reality headsets. The next step is to start finding literature that lends itself to the implementation of my project. In the back of my head I have been considering the various platforms I could use and frameworks. I would really like to avoid using an existing game engine as my project application will not need much more than a renderer to function. I am starting to lean toward implementing a Virtual Reality camera for the Game Education Framework.\r\n\r\n### *02/10/16* Refining the Topic\r\nI had a small scare when speaking with a lecturer about my topic about the validity of my research question as a technical problem. Fortunately this was resolved when I further discussed the details of my proposal and although I have not posed a technical problem it will certainly have a technical solution. I have also altered my topic to cover Adaptive User Interfaces for Motion Controlled Virtual Reality as opposed to Adaptive and Intelligent User Interfaces. This is because Adaptive User Interfaces could be applied more readily for short user sessions whereas an intelligent user interface would require a number of longer sessions. After more research there seem to be two main 'semantics' when it comes to adaptivity for user interfaces, static and dynamic. Static adaptivity focuses on updating and optimising the user experience between sessions while dynamic uses techniques to continuously adjust the interface for the user. \r\n\r\nAfter speaking with a lecturer I am starting to consider implementing my application in Unreal Engine or Unity but obviously focusing on the programming of the user interface and avoiding their systems for implementing such things. They persuaded me that this would allow more focus on the development of the adaptivity. The Playstation VR headset available for research purposes would allow me to develop for motion controlled virtual reality without competing with the many 3rd year group projects using the Vive (the other motion controlled VR platform available to us). I also emailed someone at Oculus about the potential of an Oculus Touch developer kit but have not heard back. The next step is to begin setting up a project and pipeline for Unity/Unreal and the PSVR and work out how I can develop an application without the constant need for the headset as it will be shared and I may not always have access.\r\n\r\n### *06/10/16* Evaluating existing interfaces\r\nI spent a few hours exploring the various interfaces of the available Vive games on the University's machine. I noted some interesting features. Almost every game uses highlighting to display when something will be interacted with. This creates a fairly robust sense of responsiveness when quickly performing tasks. However when many items are selectable the highlighted object isn't always the obvious one at least in Fantastic Contraption and Valve's The Lab. Another point of interest was that the majority of my errors were made when trying to pick things up from the floor or on surfaces. Although it seems counter-intuitive the interfaces hovering in the air which don't really have a real world reference are consistently easier to use. Although it does become tiring using elevated interfaces.\r\n\r\nMost of the experiences utilised only one button on the controller outside of very specific circumstances in which the controller changed to suit the interaction, i.e. the touchpad became a scroll wheel in VR to display that interacting would scroll the visible text. I thought thoroughly about how to execute the practical side of the project as well as the evaluation aspect. By creating an iteration of an interface which can then be evaluated through testing seems like the obvious choice. The first iteration would be a typical 2D interface like many of the menus found in VR games. The second iteration would be a 3D interface which utilises static adaptivity. i.e. will adapt during transition sequences like going back or forward through a menu. The final iteration would use dynamic adaptivity to create a continuously adjusting environment to accommodate as best as possible to the user.\r\n\r\nThe evaluation would be done by presenting the user with the three environments asking them to perform a series of interactions and then evaluate how usable the interfaces were. This would most likely also ask them to evaluate how much error they made using the interfaces. As well as having quality testing I plan to implement quantitative testing by gathering data on how many times attempted interactions fail or are undone. This should provide a good idea of how well the adaptive elements of the program affect the usability.\r\n\r\n### *17/10/16* Continuing evaluation and preparing application\r\nI created a timetable toward the end of last week outlining what I want to try and achieve each week until week 14. Currently I plan to complete two readings a week and a section of my proposal every two weeks. Alongside this i will begin creating a feasibility demo and continue evaluating other applications. Today I spent a few hours analysing interfaces within other applications. Most had little to no interaction, with gaze being the primary input method. The ones that did not use locomotion were by far the most comfortable while any that tried to implement non teleportation based locomotion made me feel unease within seconds.\r\n\r\nI used the Playstation VR which I was extremely impressed by. The visuals were incredibly smooth and seemed higher fidelity than those of the Vive. The tracking worked well although I did not use the move controllers. Unfortunately the design of the applications I played let it down hugely with all but one relying on translation based movement using a controller. Having thought often about my research question I am beginning to think I should focus not only on the concept of adaptive user interfaces but also on the systems that gather data to support those interfaces i.e. Error Analysis and Pattern Recognition.\r\n\r\n### *18/10/16* Notes on adaptivity\r\nReading through the paper \"Error-based Adaptive User Interfaces\" which focuses on making 2D touch interfaces more accessible I came across some terminology to describe ideas I have been developing. First of all the concept of a 'reversal', which is defined as \"the act of a user where he/she taps on a component and “quickly” reverses the action\". I can imagine these types of errors being very easy to detect and utilise. The paper also mentions the idea of user models, trying to understand how the user interacts with the interface and using that to adapt it.\r\n\r\nI also set up a project for the Vive in Unreal Engine 4 which was a very smooth process aside from some issues aligning the world floor with the real floor. I am still thinking about what complex interface I can use to demo my research. I think it needs to be something with simple interactions that utilises 3D space and encourages quick use. Maybe some kind of RTS interface would be interesting, could do some simple steering type mechanisms.\r\n\r\n### *19/10/16* Continued notes...\r\nThe paper \"Plasticity in 3D User Interfaces\" goes into some detail about the different categories of plastic user interfaces i.e. those which can be modified/modify themselves. The table below defines adaptive user interfaces as those which are adapted by the system at runtime. This is inline with the use of error-analysis which by necessity is by the system at runtime.\r\n![Table](http://i.imgur.com/c4Bsnup.png)\r\n\r\nLater in the paper it discusses the types of interactivity in 3D worlds, breaking it into 3 modes of interaction: 'object manipulation, navigation and application control'. To begin building adaptive interfaces these interactions will need to be further separated into the intentions of the user. Object manipulation could consist of transformation (i.e. movement, rotation), interaction (verbs i.e. taking, using, dropping) and perhaps meta tasks such as storing in an inventory, examining, deleting. Navigation can be broken into translation and rotation or looking. Application Control could be any manner of things but probably simplifies to user interface interaction like highlighting, selecting and undoing. By taking these individual components of user interaction and finding their points of weakness I can begin deciding how adaptation could benefit them. One example that comes to mind is when a user wants to take an object that is in 3D space but it is distant and they keep making errors. By using the context of that action the system can make an educated guess at what the user wanted to do and then suggest or execute that. The system could look at what the closest object during the failed interaction was or what the user was looking at during the interaction, it could even use previous data of patterns in interaction to predict the intended action.\r\n\r\n---\r\n\r\n###Structured Abstract Draft:\r\n\r\n####Context:\r\nVirtual reality (VR) headsets and motion controllers allow for a new depth of 3D interaction. Positional tracking allows the user to freely look around and interact much like in the real world. Unfortunately these extra degrees of freedom add more room for error and complex interfaces in VR can be tiresome. \r\n\r\n#####Aim:\r\nThis paper seeks to explore the implementation of adaptive user interfaces in VR and assess their effect on usability.\r\n\r\n#####Method:\r\nA static interface will be implemented and compared with a dynamically adaptive iteration.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}