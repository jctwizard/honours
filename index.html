<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>HonoursJournal by jctwood</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>HonoursJournal</h1>
        <p>Adaptive user interfaces for motion controlled virtual reality</p>

        <p class="view"><a href="https://github.com/jctwood/honours">View the Project on GitHub <small>jctwood/honours</small></a></p>


        <ul>
          <li><a href="https://github.com/jctwood/honours/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/jctwood/honours/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/jctwood/honours">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h3>
<a id="160916-hello-world" class="anchor" href="#160916-hello-world" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>16/09/16</em> Hello World</h3>

<p>After listening to the various pieces of advice given to us I decided the best way to find a topic for my proposal would be to find a problem I would enjoy researching and solving. Having closely followed developments in Virtual Reality over the past years I know that user interfaces still have many issues associated with them. When using any controller with positional tracking it is harder to design a single interface for every user, due to different heights, arm lengths and dexterity. Thus my preliminary concept for a topic is to make a user interface back end that can easily adapt and intelligently evolve based on the user.</p>

<h3>
<a id="260916-research" class="anchor" href="#260916-research" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>26/09/16</em> Research</h3>

<p>I spent a good few days trawling through ACM and the library search uncovering various books along with the conference, IUI (Intelligent User Interfaces). I contacted the professor who runs the conference to request the location of the previous papers and was pleasantly surprised to receive a quick reply directing me toward the proceedings of the last 21 conferences. I also began doing some user research (probably not the term) by borrowing the key to the HIVE in Abertay which is filled with various virtual reality headsets. The next step is to start finding literature that lends itself to the implementation of my project. In the back of my head I have been considering the various platforms I could use and frameworks. I would really like to avoid using an existing game engine as my project application will not need much more than a renderer to function. I am starting to lean toward implementing a Virtual Reality camera for the Game Education Framework.</p>

<h3>
<a id="021016-refining-the-topic" class="anchor" href="#021016-refining-the-topic" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>02/10/16</em> Refining the Topic</h3>

<p>I had a small scare when speaking with a lecturer about my topic about the validity of my research question as a technical problem. Fortunately this was resolved when I further discussed the details of my proposal and although I have not posed a technical problem it will certainly have a technical solution. I have also altered my topic to cover Adaptive User Interfaces for Motion Controlled Virtual Reality as opposed to Adaptive and Intelligent User Interfaces. This is because Adaptive User Interfaces could be applied more readily for short user sessions whereas an intelligent user interface would require a number of longer sessions. After more research there seem to be two main 'semantics' when it comes to adaptivity for user interfaces, static and dynamic. Static adaptivity focuses on updating and optimising the user experience between sessions while dynamic uses techniques to continuously adjust the interface for the user. </p>

<p>After speaking with a lecturer I am starting to consider implementing my application in Unreal Engine or Unity but obviously focusing on the programming of the user interface and avoiding their systems for implementing such things. They persuaded me that this would allow more focus on the development of the adaptivity. The Playstation VR headset available for research purposes would allow me to develop for motion controlled virtual reality without competing with the many 3rd year group projects using the Vive (the other motion controlled VR platform available to us). I also emailed someone at Oculus about the potential of an Oculus Touch developer kit but have not heard back. The next step is to begin setting up a project and pipeline for Unity/Unreal and the PSVR and work out how I can develop an application without the constant need for the headset as it will be shared and I may not always have access.</p>

<h3>
<a id="061016-evaluating-existing-interfaces" class="anchor" href="#061016-evaluating-existing-interfaces" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>06/10/16</em> Evaluating existing interfaces</h3>

<p>I spent a few hours exploring the various interfaces of the available Vive games on the University's machine. I noted some interesting features. Almost every game uses highlighting to display when something will be interacted with. This creates a fairly robust sense of responsiveness when quickly performing tasks. However when many items are selectable the highlighted object isn't always the obvious one at least in Fantastic Contraption and Valve's The Lab. Another point of interest was that the majority of my errors were made when trying to pick things up from the floor or on surfaces. Although it seems counter-intuitive the interfaces hovering in the air which don't really have a real world reference are consistently easier to use. Although it does become tiring using elevated interfaces.</p>

<p>Most of the experiences utilised only one button on the controller outside of very specific circumstances in which the controller changed to suit the interaction, i.e. the touchpad became a scroll wheel in VR to display that interacting would scroll the visible text. I thought thoroughly about how to execute the practical side of the project as well as the evaluation aspect. By creating an iteration of an interface which can then be evaluated through testing seems like the obvious choice. The first iteration would be a typical 2D interface like many of the menus found in VR games. The second iteration would be a 3D interface which utilises static adaptivity. i.e. will adapt during transition sequences like going back or forward through a menu. The final iteration would use dynamic adaptivity to create a continuously adjusting environment to accommodate as best as possible to the user.</p>

<p>The evaluation would be done by presenting the user with the three environments asking them to perform a series of interactions and then evaluate how usable the interfaces were. This would most likely also ask them to evaluate how much error they made using the interfaces. As well as having quality testing I plan to implement quantitative testing by gathering data on how many times attempted interactions fail or are undone. This should provide a good idea of how well the adaptive elements of the program affect the usability.</p>

<h3>
<a id="171016-continuing-evaluation-and-preparing-application" class="anchor" href="#171016-continuing-evaluation-and-preparing-application" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>17/10/16</em> Continuing evaluation and preparing application</h3>

<p>I created a timetable toward the end of last week outlining what I want to try and achieve each week until week 14. Currently I plan to complete two readings a week and a section of my proposal every two weeks. Alongside this i will begin creating a feasibility demo and continue evaluating other applications. Today I spent a few hours analysing interfaces within other applications. Most had little to no interaction, with gaze being the primary input method. The ones that did not use locomotion were by far the most comfortable while any that tried to implement non teleportation based locomotion made me feel unease within seconds.</p>

<p>I used the Playstation VR which I was extremely impressed by. The visuals were incredibly smooth and seemed higher fidelity than those of the Vive. The tracking worked well although I did not use the move controllers. Unfortunately the design of the applications I played let it down hugely with all but one relying on translation based movement using a controller. Having thought often about my research question I am beginning to think I should focus not only on the concept of adaptive user interfaces but also on the systems that gather data to support those interfaces i.e. Error Analysis and Pattern Recognition.</p>

<h3>
<a id="181016-notes-on-adaptivity" class="anchor" href="#181016-notes-on-adaptivity" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>18/10/16</em> Notes on adaptivity</h3>

<p>Reading through the paper "Error-based Adaptive User Interfaces" which focuses on making 2D touch interfaces more accessible I came across some terminology to describe ideas I have been developing. First of all the concept of a 'reversal', which is defined as "the act of a user where he/she taps on a component and “quickly” reverses the action". I can imagine these types of errors being very easy to detect and utilise. The paper also mentions the idea of user models, trying to understand how the user interacts with the interface and using that to adapt it.</p>

<p>I also set up a project for the Vive in Unreal Engine 4 which was a very smooth process aside from some issues aligning the world floor with the real floor. I am still thinking about what complex interface I can use to demo my research. I think it needs to be something with simple interactions that utilises 3D space and encourages quick use. Maybe some kind of RTS interface would be interesting, could do some simple steering type mechanisms.</p>

<h3>
<a id="191016-continued-notes" class="anchor" href="#191016-continued-notes" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>19/10/16</em> Continued notes...</h3>

<p>The paper "Plasticity in 3D User Interfaces" goes into some detail about the different categories of plastic user interfaces i.e. those which can be modified/modify themselves. The table below defines adaptive user interfaces as those which are adapted by the system at runtime. This is inline with the use of error-analysis which by necessity is by the system at runtime.
<img src="http://i.imgur.com/c4Bsnup.png" alt="Table"></p>

<p>Later in the paper it discusses the types of interactivity in 3D worlds, breaking it into 3 modes of interaction: 'object manipulation, navigation and application control'. To begin building adaptive interfaces these interactions will need to be further separated into the intentions of the user. Object manipulation could consist of transformation (i.e. movement, rotation), interaction (verbs i.e. taking, using, dropping) and perhaps meta tasks such as storing in an inventory, examining, deleting. Navigation can be broken into translation and rotation or looking. Application Control could be any manner of things but probably simplifies to user interface interaction like highlighting, selecting and undoing. By taking these individual components of user interaction and finding their points of weakness I can begin deciding how adaptation could benefit them. One example that comes to mind is when a user wants to take an object that is in 3D space but it is distant and they keep making errors. By using the context of that action the system can make an educated guess at what the user wanted to do and then suggest or execute that. The system could look at what the closest object during the failed interaction was or what the user was looking at during the interaction, it could even use previous data of patterns in interaction to predict the intended action.</p>

<h3>
<a id="211016-abstract-draft" class="anchor" href="#211016-abstract-draft" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>21/10/16</em> Abstract Draft</h3>

<p>Virtual reality allows for intuitive 3D interaction using head and hand tracking. However many traditional 2D interfaces become awkward and cumbersome when translated to virtual reality (VR). This is due to the combination of fine input afforded by motion controls and the extra degrees of movement native to 3D space. One possible way to retain the usability of these interfaces is to have them adapt as they are interacted with. This paper seeks to explore the implementation of adaptive user interfaces in VR and assess their effect on usability. By using error analysis and pattern recognition the interface could try to predict or suggest actions the user may be trying to perform. Two iterations of a 3D interface, one with adaptation, will be used to compare the usability in various ways. The result of this comparison should demonstrate the benefit of adaptive user interfaces for virtual reality. If so then complex tasks currently relegated to 2D interfaces could be brought into the intuitive world of VR.</p>

<h3>
<a id="251016-presentation" class="anchor" href="#251016-presentation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>25/10/16</em> Presentation</h3>

<p>Completed my presentation to the class and received some interesting questions. Mostly on whether the adaptation would take place dynamically at run time and if so how does that affect a new user's ability to learn the interface. It is something I will have to consider during development. I meet my supervisor later today so will try and get through at least the first half of my proposal draft if not more. Also discovered that the abstract needs to be structured, i.e. separated into its components, context, aim etc. Managed to get down the beginnings of my proposal, introduction and context. It is really helping me solidify the thoughts in my head and hopefully once it is finished I can use the iterations to begin refining the techniques I wish to employ.</p>

<p>I have just finished the first draft of my proposal. Although it is incoherent at times it has outlined which areas I need to research more particularly the techniques and methodology of how I will go about implementing the interfaces. I need to make my template interface very concrete and feasible with the techniques for adaptive ui nailed down. A few more references out with adaptive ui would be useful and help support the context. Currently I speak too much about the background of general ui rather than the background of specifically adaptive ui.</p>

<p>About to have my first meetings with my supervisor so hopefully I will glean some action to take for the next week. It is definitely possible that by the end of this week I can have something polished up that I'd be happy to submit. As they say the first step is always the hardest. I also plan to have my other module's coursework in a state which I can be happy with. It should provide some much needed rest between bouts of writing and research.</p>

<h3>
<a id="271016-research-on-vr-ui" class="anchor" href="#271016-research-on-vr-ui" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>27/10/16</em> Research on VR UI</h3>

<p>Having finished my first draft I decided to go back and research the areas I could not back up. These were mainly usability and ux design in VR. I found some really great resources including a Valve GDC talk which led me to Fitt's law about speed of pointer use in correlation to size of target. This really supports my view that adding a third dimension can make navigation more challenging at a distance. I also found a research paper concluding that 3D tracked controllers are no more fatiguing than using a mouse to navigate 3D environments. I made many notes and am about to rewrite my proposal before passing it on to my supervisor for corrections.</p>

<h3>
<a id="021116-proposal" class="anchor" href="#021116-proposal" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>02/11/16</em> Proposal</h3>

<p>Over the weekend I polished my proposal and sent it to my supervisor. Their feedback focused on adding visual representations of harder to grasp concepts. I used the 3D colour palette from tilt brush and the 2D colour palette in unreal engine 4. I would still like to find some diagram of interactions to show how a 3D interface could be seen as more intuitive than a 2D counterpart. We were also given more detailed information about the feasibility demos which we need to have 3 'artefacts' for. Currently I plan to have a design document outlining the components of the interface, a basic version of the analytics used to gather quantitative data and the interface itself in unreal engine 4. Today I plan to create a schedule for the next few months which I can include in the proposal.</p>

<h3>
<a id="071116-submission" class="anchor" href="#071116-submission" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>07/11/16</em> Submission</h3>

<p>I spent today finalising the references in my proposal and expanding on the summary and evaluation. It will be a relief to have submitted it tonight and start working on the implementation for the feasibility demo the rest of the week. </p>

<h3>
<a id="211116-feasability-demo" class="anchor" href="#211116-feasability-demo" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>21/11/16</em> Feasability Demo</h3>

<p>I have spent the past weeks working on my other AR module which has also fed into ideas for my honours application. Today I am setting up a new UE4 4.14 project with basic vr interaction and the beginnings of the demo. By the end of the week I hope to have the various manipulations working and the analytics outputting. Then it is on to building the actual gameplay the player will be completing. The more I think about it the more I realise it will need to be something that can be pressurised. A series of tasks that need to be done rapidly. This will allow me to force the player to use the interface quickly and not cautiously as it would be in fast complex games. I am still settling on the exact gameplay.</p>

<h3>
<a id="011216-vr-in-c-in-ue4" class="anchor" href="#011216-vr-in-c-in-ue4" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>01/12/16</em> VR in C++ in UE4</h3>

<p>With my other coursework handed in I have begun implementing the basic VR controller in unreal engine. In order to allow multiple types of interaction model later on I am implementing everything in C++ using the VR template blueprints as a guide for the undocumented VR api. It took a while to find the correct includes to get access to the VR functions but it seems to be in order. I booked the HIVE to test my progress on saturday so today and tomorrow will be spent finishing the implementation in C++.</p>

<p>Another benefit of C++ is that I can use the standard libraries to read and write files for easy analytics. I currently write to a text file stored in the project named with the timestamp the session was started at. Inside the text file I store every time the user makes an action, where they were when they made the action and whether it was successful along with a description. This should allow me to later visualise in 3D the movement and actions of the players and in theory create a heat map of all the testers interactions and movement later down the line.</p>

<h3>
<a id="031216-evaluation-questions" class="anchor" href="#031216-evaluation-questions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>03/12/16</em> Evaluation Questions</h3>

<p>I stumbled across a great guide for usability evaluation called SUS (System Usability Scale). The questions are fairly vague but ask the questions I am looking to ask in ways that will hopefully help draw out the user's feelings on the matter. Each question seems to mirror the last giving the user a positively phrased question like, "was it easy to use?" next to a negatively phrased but similar question, "was it cumbersome to use?". This will hopefully give a good indication of how people are finding it and remove any bias they have toward giving positive answers. Other than directly asking about usability the questions also cover how easy the system is to learn and how much the system made sense, was it too complex? etc. I am still trying to pin down exactly how the questions will be asked in regards to the various adaptive techniques being analysed.</p>

<p>Testing the VR C++ implementation just now, hopefully it works as intended and will provide a good base to start experimenting with the interfaces.</p>

<h3>
<a id="061216-data-visualisation" class="anchor" href="#061216-data-visualisation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>06/12/16</em> Data Visualisation</h3>

<p>I worked on visualising the data being output by the vr pawn. This involved creating a spline component using the data points read from a text file. Each data point contains its position which is used to move a mesh and visualise the path of the player's hands. The next step is to have the description of actions and timestamp implemented into the visualisation before building heatmaps from the data to see where the most errors were made. Doing this in 3D will be interesting and currently the visualisation is a static non inter-actable demo. </p>

<h3>
<a id="170117-research" class="anchor" href="#170117-research" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>17/01/17</em> Research</h3>

<p>I took a break to work on other projects over the holidays and have been researching the developments in VR ui over the past week. CES happened and a few new headsets powered by microsoft holographic were announced. These tend to have hand tracking and inisde out room positional tracking which removes the need for motion controllers or external sensors. Apparently they will be sold for a lower price than oculus/vive with a higher resolution and lower min specs which altogether seems like a good move. I will definitely be sticking with motion controlled input for now but I can see a simple 'click' gesture or the tap that hololens uses would maybe allow my system to be ported over to any of those other platforms. The more I watch and use VR the more I realise interaction at a distance (not the spooky kind) is where may of the issues lie. The next biggest area of error is dense interaction where there are maybe five objects in close proximity and you want to select only one. Although I will start with the error analysis that I planned initially I am thinking maybe some kind of 3D picker tool that allows objects or elements to have a kind of pin that expands out as you approach it making it easier to select. I also want to look into more gaze based interaction although I will not use a gaze tracking headset like teh FOVE I can interpret the direction the user is facing as being their gaze. One game that is using this very well is the management game Tethered. It has some very interesting interfaces. An interesting game very similar to a prototype I made a while back is VRISH DREAMS I think it is called, by Twisted Oak. I will continue my research this week while building systems for laser pointer based interaction and improving error analysis. </p>

<h3>
<a id="170117-laser-pointer" class="anchor" href="#170117-laser-pointer" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>19/01/17</em> Laser Pointer</h3>

<p>After some more research on interfaces, one of the most interesting examples being ModBox which uses a tilt brush style palette and pointer interface, to keep track of these I am using a youtube playlist and refworks. The next step for implementation is to get the laser pointer working for interaction with far away objects.</p>
      
<h3>
<a id="220117-control-mapping" class="anchor" href="#220117-control-mapping" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>22/01/17</em> Control Mapping</h3>

<p>I found a useful controller mapping for the vive in ue4 after struggling with the input names for the touchpad. It turns out it is called 'thumbstick', solving that let me test the laser pointer i implemented last week and it works well although I miss very often as i have found in other games. The trace doesn't seem to work when angled below horizontal pointing upward which is strange. I also had a very useful link given to me by a colleague that breaks down the interaction design that Sony London used to design their VR experience, London Heist. It is one of the standout VR experiences I have played in terms of intuitive interaction and it also had me extremely immersed. <a href="http://www.gdcvault.com/play/1023185/Maths-to-Mechanics-Using-Mathematical">link to it here.</a> The issue with the laser pointer turned out to be to do with the grab sphere intersecting and overriding the laser pointer. Now the controller uses a Grab Method enum to track which method of grabbing you are using and switches between them currently Sphere and Laser.
<img src="http://i.imgur.com/jgdRuzt.png" alt="Mapping"></p>
     
<h3>
<a id="220117-data-visualiation" class="anchor" href="#220117-data-visualiation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>22/01/17</em> Data Visualisation</h3>

<p>After getting the laser pointers to function I decided to look at improving the data visualiser, if I continually improve on it while I add features to the base interaction it should allow me to get a better idea of where the majority of errors are made and allow me to quickly perform user testing without worrying too much about taking notes of every user interaction. I realise that without visualising the scene the user is interacting with it will be hard to really understand what is going on so the next step seems to be visualising objects that the user interacts with. By only updating their position when they move I should get a good representation of the scene without having to store a ton of information. I plan to use an id system to store each object in the scene and then track it for position changes in order to visualise them individually. I also plan to begin visualising the actions of the user perhaps using text and a sonar type pulse coloured to show success vs failure. </p>

<h3>
<a id="240117-data-gathering" class="anchor" href="#240117-data-gathering" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>22/01/17</em> Data Gathering</h3>

<p>After making improvements to the data visualisation yesterday I realised that to get any real idea of what the player is doing i not only have to begin tracking object locations but I will also need to track the player's headset position and direction and do all of these things on a timestep. The actions will be recorded separately as they will be overlaid onto the time stepped objects. To do this i will need both a more accurate timer, right now it only outputs the seconds. I will also need a better way of saving and storing data, currently it is done using custom formatting that is hardcoded and difficult to visualise at the other end. I have never actually used xml markdown but discovered unreal has it's own simple parser for it and a more complex one called FastXml which uses a callback system. I plan to use the simple version to store data as nodes with information about the object the node belongs to as well as its position rotation etc. this will give me a better visualisation and make it more extensible later down the line. </p>

<h3>
<a id="300117-bugs" class="anchor" href="#300117-bugs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>30/01/17</em> Bugs</h3>

<p>After a break over the weekend I came back to the projet to fix the last bugs with the data gathering/visualisation system. Although I was not in the UK I was able to use my friend in France's Vive to test out my systems. One major issue revolved around the use of Unreal's Begin Destroy method which I had hoped to use to save the xml file upon the exiting of the application. However for an unknown reason this method is called immediately even before Begin Play and so the file attempted to save before even being created. Once this was fixed xml files were being created but no data was stored in them even though the new system should store the transforms of tracked objects each tick. The issue was due to a threshold value I was using so as to only store data when something had moved a certain amount. The stored positions to check against were constantly being updated which meant nothing ever moved the threshold value in a single frame. With this fixed only some minor path issues with the loading occurred. Now I have a fully functioning data gatherer and visualiser that only needs a simple component added to tracked objects. I'm considering also making this a tag so as to allow anything to be tracked without a component required. A component would still be required for sending events. Later I will come back to the visualisation and improve it to use a heatmap and graphing but the data will not change before then. The next few days before a presentation to my supervisor will be spent adding the first adaptive elements. I want these to be actor components that affest the transform of any element in the scene say a ui. And extensions to the motion controller code to store errors made and use them along with gaze data to make educated guesses about intended actions. Hopefully it goes smoothly and the next post is filled with new ideas.</p>
 
<h3>
<a id="010217-adaptive" class="anchor" href="#010217-adaptive" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>01/02/17</em> Adaptive</h3>

<p>I moved onto adding the adaptivity to the systems starting with an error based system for determining which object a user is trying to select when using the laser. I also added button mappings for toggling this on or off indicated by the colour of the laser. Currently if it fails to use the laser the system then sweeps with a sphere selecting the closest swept actor to the laser using a simple dot product calculation to project the point onto the ray and find its tangent.</p>                        

<h3>
<a id="060217-sweep-vs-iteration" class="anchor" href="#060217-sweep-vs-iteration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>06/02/17</em> Sweep vs Iteration</h3>

<p>After adding the adaptive version of the laser grab which detects when the user has made an error i.e. 'missed' whatever they tried to grab, The sphere sweep i used to detect any actors near the laser introduced a minor latency when the user missed. I implemented a simple base class for all interactables and used the getactorsofclass method before then using some basic maths to get the distance to the laser. This should be much faster and I will test it tomorrow in the lab. I leave for 5 days on wednesday but will try to add the basic scaling of interactable elements and the gaze based assistance. During the supervisor demo the idea that the adaptive laser grabbing was more effective and accurate was confirmed and they told me afterwards it was far easier for them to grab things with the laser when in the adaptive mode. Created a simple dissertation template too which I should be able to start slowly adding too.</p>

<h3>
<a id="150217-adaptive-scaling" class="anchor" href="#150217-adaptive-scaling" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>15/02/17</em> Adaptive Scaling</h3>

<p>I implemented some simple adaptive scaling based on proximity to the laser using the same dot algorithm as in the previous adaptive techniques. It immediately feels far more comfortable for the static ui but the rapidly resizing overlap sphere feels very unpredictable with any moving elements. I think it could be very useful and perhaps combined with gaze techniques could make things in static uis easier to grab. I had a short trip and so am just getting back into the swing of things once  again. I did have a breakthrough in my head about how to present the techniques and the work is almost all there already. I will have two interfaces, the 3d objects that slide down, and then a static interface. During both the user will be asked to place coloured birds in matching boxes under a time limit. Once the time limit is reached they come out of the headset to fill in a questionnaire and have the option to opt out. They repeat twice for each technique. There will be three. normal laser, adaptive laser and adaptive scaling. This allows me to get an idea of what the user finds helpful without overloading them and provides a solid base for comparison. I am still unsure whether a simple interaction tutorial should take place before hand.</p>
 </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/jctwood">jctwood</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>

  </body>
</html>
